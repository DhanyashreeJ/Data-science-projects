{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOUSE PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import xticks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "house = pd.read_csv(\"train.csv\")\n",
    "house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"SalePrice\" is the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the target variable \"SalePrice\"\n",
    "house[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great!! There are no negative values in the dataset for sale price which is good**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the distribution of saleprice\n",
    "sns.distplot(house.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saleprice seems to be skewed, This need to be handled else this will adversly impact our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop Id because its of no use to us\n",
    "house.drop(\"Id\",1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display the variables with more than 0 null values\n",
    "null_cols = []\n",
    "for col in house.columns:\n",
    "    if house[col].isnull().sum() > 0 :\n",
    "        print(\"Column\",col, \"has\", house[col].isnull().sum(),\"null values\")    \n",
    "        null_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for col in house.columns:\n",
    "    if house[col].dtypes == 'O':\n",
    "        house[col] = house[col].replace(np.nan,\"None\")\n",
    "    else:\n",
    "        house[col] = house[col].replace(np.nan,house[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making list of date variables\n",
    "yr_vars = []\n",
    "for col in house.columns:\n",
    "    if \"Yr\" in col or \"Year\" in col:\n",
    "        yr_vars.append(col)\n",
    "\n",
    "yr_vars = set(yr_vars)\n",
    "yr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Make a note of the trend of sale price with the field \"YrSold\", it shows a decreasing trend which seems unreal in real state scenario, price is expected to increase as the time passes by, but here it shows opposite. Let's handle this by creating \"Age\" variables from these variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating age variables\n",
    "house['HouseAge'] =  house['YrSold'] - house['YearBuilt']\n",
    "# age of master after remodelling\n",
    "house['RemodAddAge'] = house['YrSold'] - house['YearRemodAdd']\n",
    "# creating age of the garage from year built of the garage to the sale of the master\n",
    "house['GarageAge'] = house['YrSold'] - house['GarageYrBlt'] \n",
    "\n",
    "# lets drop original variables\n",
    "house.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check variation in the feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets firs create seperate lists of categorical and numeric columns\n",
    "cat_vars = []\n",
    "num_vars = []\n",
    "for col in house.columns.drop(\"SalePrice\"):\n",
    "    if house[col].dtypes == 'O':\n",
    "        cat_vars.append(col)\n",
    "    else:\n",
    "        num_vars.append(col)\n",
    "\n",
    "#lets check the lists created.\n",
    "print(\"List of Numeric Columns:\",num_vars)\n",
    "print(\"\\n\")\n",
    "print(\"List of Categorical Columns:\",cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's further seperate the numeric features into continous and discrete numeric features\n",
    "num_cont = []\n",
    "num_disc = []\n",
    "for col in num_vars:\n",
    "    if house[col].nunique() > 25: # if variable has more than 25 different values, we consider it as continous variable\n",
    "        num_cont.append(col)\n",
    "    else:\n",
    "        num_disc.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low variance:**\n",
    "* **LowQualFinSF**\n",
    "* **BsmtHalfBath**\n",
    "* **KitchenAbvGr**\n",
    "* **3SsnPorch**\n",
    "* **PoolArea**\n",
    "* **MiscVal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check for the variance in the categorical columns present in the dataset\n",
    "plt.figure(figsize = (20,200))\n",
    "for idx,col in enumerate(cat_vars):\n",
    "    plt.subplot(22,2,idx+1)\n",
    "    ax=sns.countplot(house[col])\n",
    "    xticks(rotation=45)\n",
    "    #for p in ax.patches:\n",
    "    #    ax.annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following variables seems to have low variance:**\n",
    "\n",
    "* MSZoning\n",
    "* Street,\n",
    "* Alley\n",
    "* LandContour,\n",
    "* Utilities,\n",
    "* LotConfig\n",
    "* Condition1\n",
    "* LandSlope\n",
    "* Condition2,\n",
    "* BldgType\n",
    "* RoofStyle\n",
    "* RoofMatl\n",
    "* ExterCond\n",
    "* BsmtCond\n",
    "* BsmtFinType2\n",
    "* Heating\n",
    "* CentralAir\n",
    "* Electrical\n",
    "* Functional\n",
    "* GarageQual\n",
    "* GarageCond\n",
    "* PavedDrive\n",
    "* PoolQC\n",
    "* Fence\n",
    "* MiscFeature\n",
    "* SaleType\n",
    "* SaleCondition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the variables identified above as they have low variance\n",
    "low_var_num_cont = ['MasVnrArea','BsmtFinSF2','2ndFlrSF','EnclosedPorch','ScreenPorch']\n",
    "\n",
    "low_var_num_disc = ['LowQualFinSF','BsmtHalfBath','KitchenAbvGr','3SsnPorch','PoolArea','MiscVal']\n",
    "\n",
    "low_var_cat_vars = ['MSZoning','Alley','LandContour','Utilities','LotConfig','Condition1','LandSlope','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond','BsmtFinType2','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','SaleType','SaleCondition','Street','Fence','MiscFeature']\n",
    "\n",
    "house.drop(low_var_num_cont,1,inplace= True)\n",
    "house.drop(low_var_num_disc,1,inplace= True)\n",
    "house.drop(low_var_cat_vars,1,inplace= True)\n",
    "\n",
    "num_cont = list(set(num_cont)-set(low_var_num_cont))\n",
    "num_disc = list(set(num_disc)-set(low_var_num_disc))\n",
    "cat_vars = list(set(cat_vars)-set(low_var_cat_vars))\n",
    "       \n",
    "num_vars = num_cont + num_disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets handle Skewness before moving to Bi-Variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets handle skewness in saleprice, lets take log to get normal distribution\n",
    "house.SalePrice = np.log(house.SalePrice)\n",
    " \n",
    "# lets check the distribution of saleprice again\n",
    "sns.distplot(house.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SalePrice looks good now, lets handle other numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the log of numeric variables to hanlde skewness\n",
    "num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n",
    "for col in num_features:\n",
    "    house[col] = np.log(house[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop MSSubClass, YrSold & MoSold as they have no impact on SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the variables\n",
    "house.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)\n",
    "\n",
    "num_disc = list(set(num_disc)-set(['MSSubClass','YrSold','MoSold']))\n",
    "num_vars = list(set(num_vars)-set(['MSSubClass','YrSold','MoSold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers in the dataset, these will be treated in the data engineering section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set[\"SalePrice\"] = train_sp\n",
    "# lets check the variables\n",
    "#num_vars = []\n",
    "#for col in train_set.columns:\n",
    "#    if train_set[col].dtypes != 'O':\n",
    "#        num_vars.append(col)\n",
    "\n",
    "for col in num_vars:\n",
    "    print(house[col].describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.95,0.99]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# lets handle the outliers\n",
    "q3 = house['OpenPorchSF'].quantile(0.99)\n",
    "house = house[house.OpenPorchSF <= q3]\n",
    "    \n",
    "q3 = house['GarageArea'].quantile(0.99)\n",
    "house = house[house.GarageArea <= q3]\n",
    "\n",
    "q3 = house['TotalBsmtSF'].quantile(0.99)\n",
    "house = house[house.TotalBsmtSF <= q3]\n",
    "\n",
    "q3 = house['BsmtUnfSF'].quantile(0.99)\n",
    "house = house[house.BsmtUnfSF <= q3]\n",
    "\n",
    "q3 = house['WoodDeckSF'].quantile(0.99)\n",
    "house = house[house.WoodDeckSF <= q3]\n",
    "\n",
    "q3 = house['BsmtFinSF1'].quantile(0.99)\n",
    "house = house[house.BsmtFinSF1 <= q3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read the test dataset, we will apply all the feature engineering operations on test set as well\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# save \"Id\" in a variable and drop the column (as we have already dropped from train dataset)\n",
    "tid = test_set.Id\n",
    "test_set.drop(\"Id\",1,inplace = True)\n",
    "\n",
    "# save SalePrice to a variable and drop it from training dataset as test dataset does not have this column\n",
    "train_sp = house.SalePrice\n",
    "house.drop(\"SalePrice\",1,inplace=True)\n",
    "\n",
    "# all missing values for the categorical columns will be replaced by \"None\"\n",
    "# all missing values for the numeric columns will be replaced by median of that field\n",
    "for col in test_set.columns:\n",
    "    if test_set[col].dtypes == 'O':\n",
    "        test_set[col] = test_set[col].replace(np.nan,\"None\")\n",
    "    else:\n",
    "        test_set[col] = test_set[col].replace(np.nan,test_set[col].median())\n",
    "\n",
    "\n",
    "# creating age of the master from year built to the sale of the master\n",
    "test_set['HouseAge'] =  test_set['YrSold'] - test_set['YearBuilt']\n",
    "# age of master after remodelling\n",
    "test_set['RemodAddAge'] = test_set['YrSold'] - test_set['YearRemodAdd']\n",
    "# creating age of the garage from year built of the garage to the sale of the master\n",
    "test_set['GarageAge'] = test_set['YrSold'] - test_set['GarageYrBlt'] \n",
    "\n",
    "# lets drop original variables\n",
    "test_set.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)\n",
    "        \n",
    "        \n",
    "# skewness in test set\n",
    "# taking the log of numeric variables to hanlde skewness\n",
    "num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n",
    "for col in num_features:\n",
    "    test_set[col] = np.log(test_set[col])\n",
    "\n",
    "            \n",
    "test_set.drop(low_var_num_cont,1,inplace= True)\n",
    "test_set.drop(low_var_num_disc,1,inplace= True)\n",
    "test_set.drop(low_var_cat_vars,1,inplace= True)\n",
    "\n",
    "test_set.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)        \n",
    "        \n",
    "\n",
    "# merge the two datasets\n",
    "master=pd.concat((house,test_set)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to perform linear regression, we need to convert categorical variables to numeric variables.\n",
    "\n",
    "# We have ordinal variables present in the dataest, lets treat them first:\n",
    "master['ExterQual'] = master['ExterQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\n",
    "master['BsmtQual'] = master['BsmtQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\n",
    "master['BsmtExposure'] = master['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0})\n",
    "master['BsmtFinType1'] = master['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'None':0})\n",
    "master['HeatingQC'] = master['HeatingQC'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\n",
    "master['KitchenQual'] = master['KitchenQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\n",
    "master['GarageFinish'] = master['GarageFinish'].map({'Fin':3,'RFn':2,'Unf':1,'None':0})\n",
    "master['FireplaceQu'] = master['FireplaceQu'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets create dummy variables for the remaining cateogorical variables\n",
    "cat_vars = []\n",
    "for col in master.columns:\n",
    "    if master[col].dtypes == 'O':\n",
    "        cat_vars.append(col)\n",
    "\n",
    "# convert into dummies\n",
    "master_dummies = pd.get_dummies(master[cat_vars], drop_first=True)\n",
    "\n",
    "# drop categorical variables \n",
    "master.drop(cat_vars,1,inplace = True)\n",
    "\n",
    "# concat dummy variables with X\n",
    "master = pd.concat([master, master_dummies], axis=1)\n",
    "\n",
    "# lets check the shape of the final dataset\n",
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have perfomed all the necessary operations on the train and test datasets, time to sperate the two sets again\n",
    "train_set = master[:1372]\n",
    "\n",
    "test_set = master[1372:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "y = train_sp.reset_index(drop=True)\n",
    "\n",
    "scaler.fit(train_set)\n",
    "X = scaler.transform(train_set)\n",
    "\n",
    "# transform the train and test set, and add on the Id and SalePrice variables\n",
    "X = pd.DataFrame(X,columns = train_set.columns).reset_index(drop=True)\n",
    "X.head()\n",
    "\n",
    "scaler.fit(test_set)\n",
    "test_set = scaler.transform(test_set)\n",
    "test_set = pd.DataFrame(test_set,columns = train_set.columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improting the PCA module\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(svd_solver='randomized', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's apply PCA\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of PCA components.It would be the same as the number of variables\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the variance ratios\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From Scree plot we can conclude that we 60 PCs can explain around 90% variation of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using incremental PCA for efficiency - saves a lot of time on larger datasets\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_final = IncrementalPCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(pca_final.fit_transform(X))\n",
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to get an intercept\n",
    "X_train_sm = sm.add_constant(df_pca)\n",
    "\n",
    "# train the model\n",
    "lr = sm.OLS(y, X_train_sm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a summary operation lists out all the different parameters of the regression line fitted\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on training dataset\n",
    "y_train_pred = lr.predict(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared = r2_score(y_train_pred, y)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, y_train_pred))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make predictions on the test dataset\n",
    "\n",
    "test_pca = pd.DataFrame(pca_final.fit_transform(test_set))\n",
    "\n",
    "test_pca_sm = sm.add_constant(test_pca)\n",
    "y_test_pred = lr.predict(test_pca_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression produced good resuls, but, lets try Random Forest as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# training the model\n",
    "regr = RandomForestRegressor(n_estimators=50,random_state=0,n_jobs=1)\n",
    "regr.fit(df_pca,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make prediction on training dataset\n",
    "y_train_pred = regr.predict(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared = r2_score(y_train_pred, y)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, y_train_pred))\n",
    "rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearly, performance is better with Random Forest,  RF model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make prediction on test dataset\n",
    "y_test_pred = regr.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
